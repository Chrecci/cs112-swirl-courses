- Class: meta
  Course: CS112
  Lesson: Session 10 Random Forests
  Author: Michael Chen
  Type: Standard
  Organization: Minerva University
  Version: 2.4.5
  
- Class: text
  Output: Welcome to the Swirl lesson for CS112 Session 9, Random Forests. 
      Please note that you can use the function rpt() to go back to the
      previous question. To end your Swirl session, you can type bye().
      After you exit a lesson, you can always resume a lesson by typing 
      swirl() again. However, make sure you use the same name you always use!
      
- Class: text
  Output: In this lesson, we will build upon the last one about decision trees, and grow a forest of decision trees!
      If you have not done the previous lesson, it is strong recommended to do that before starting this one.

- Class: cmd_question
  Output: Let's start by loading the necessary library for growing a random forest. 
      Load the "randomForest" library. Install it if you haven't done so.
  CorrectAnswer: library(randomForest)
  AnswerTests: any_of_exprs('library(randomForest)')
  Hint: To install, install.packages("randomForest"). To load, library(randomForest).

- Class: text
  Output: Our trusty lalonde dataset is automatically loaded in the beginning of the session.
      It is also appropriately separated into training set (n=345) and test set (n=100). 

- Class: cmd_question
  Output: Let's set a seed to make sure our trees turns out the same way. Set seed to 112.
  CorrectAnswer: set.seed(112)
  AnswerTests: any_of_exprs('set.seed(112)')
  Hint: set.seed(112)

- Class: mult_question
  Output: First, we will apply bagging to the dataset. Note that in the lalonde dataset, there are 12 variables in total.
      If we use re75 as the dependent variable and the rest (except re78) as independent variables, 
      how many independent variables are there?
  AnswerChoices: 12;11;10;9
  CorrectAnswer: "10"
  AnswerTests: omnitest(correctVal='10')
  Hint: We can't use re75 and re78.


- Class: cmd_question
  Output: Use the randomForest() function, which is similar in format as tree(), to apply bagging.
      Save the forest to an object called bag.lalonde.
      Here's the expression we'll use, randomForest(re75~.-re78, data=lalonde.train, mtry=10, importance = TRUE).
      mtry specifies the number of variables that can be used to classify at each node.
      importance allows us to see the relative influence of each variable.
  CorrectAnswer: bag.lalonde <- randomForest(re75~.-re78, data=lalonde.train, mtry=10, importance = TRUE)
  AnswerTests: any_of_exprs('bag.lalonde <- randomForest(re75~.-re78, data=lalonde.train, mtry=10, importance = TRUE)')
  Hint: Just copy and paste the expression and save it to an object called bag.lalonde.

- Class: cmd_question
  Output: Let's take a look at our bagging model.
  CorrectAnswer: bag.lalonde
  AnswerTests: any_of_exprs('bag.lalonde')
  Hint: Just type bag.lalonde.

- Class: text
  Output: We can see that we grew 500 trees, and at each split, all 10 independent variables are considered.
      The mean squared error and R^2 value are also reported.

- Class: cmd_question
  Output: Let's calculate the rmse value of this model using the test set. First, let's generate the predictions.
      Save the predictions to an object called bag.test.pred.
  CorrectAnswer: bag.test.pred <- predict(bag.lalonde, lalonde.test)
  AnswerTests: any_of_exprs('bag.test.pred <- predict(bag.lalonde, lalonde.test)')
  Hint: Remember we put the model and the test set as arguments in predict().

- Class: cmd_question
  Output: Now we can use the calc_rmse function (already loaded) to calculate the training set error.
      Save the result to an object called rmse.bag.test.
  CorrectAnswer: rmse.bag.test <- calc_rmse(lalonde.test$re75, bag.test.pred)
  AnswerTests: any_of_exprs('rmse.bag.test <- calc_rmse(lalonde.test$re75, bag.test.pred)')
  Hint: Change this, calc_rmse(ytrue, ypred)


- Class: mult_question
  Output: Now, we will apply random forests to the dataset. Note that in the lalonde dataset, there are 12 variables in total.
      If we use re75 as the dependent variable and the rest (except re78) as independent variables, 
      how many variables can we use at each split?
      (Note that the heuristic is to use sqrt(number of variables))
  AnswerChoices: 11;10;9;4;3;2
  CorrectAnswer: "3"
  AnswerTests: omnitest(correctVal='3')
  Hint: Square root of 10 is ~3. 

- Class: cmd_question
  Output: Now, use the randomForest() function to make a random forest.
      Save the forest to an object called rf.lalonde.
      (Hint, you only need to change one parameter in from the bagging model.)
  CorrectAnswer: rf.lalonde <- randomForest(re75~.-re78, data=lalonde.train, mtry=3, importance = TRUE)
  AnswerTests: any_of_exprs('rf.lalonde <- randomForest(re75~.-re78, data=lalonde.train, mtry=3, importance = TRUE)')
  Hint: Change the expression and save it to an object called rf.lalonde.

- Class: cmd_question
  Output: Let's take a look at our random forest model.
  CorrectAnswer: rf.lalonde
  AnswerTests: any_of_exprs('rf.lalonde')
  Hint: Just type rf.lalonde.

- Class: cmd_question
  Output: We can also look at the respective importance of each variable. Input importance(rf.lalonde).
      The %IncMSE value indicates the change in MSE when the variable is removed.
      Thus, the larger the %IncMSE, the more important the variable.
  CorrectAnswer: importance(rf.lalonde)
  AnswerTests: any_of_exprs('importance(rf.lalonde)')
  Hint:

- Class: cmd_question
  Output: The importance can also be visualized by the varImpPlot(). Input varImpPlot(rf.lalonde)
  CorrectAnswer: varImpPlot(rf.lalonde)
  AnswerTests: any_of_exprs('varImpPlot(rf.lalonde)')
  Hint:

- Class: cmd_question
  Output: Let's calculate the rmse value of this model using the test set. First, let's generate the predictions.
      Save the predictions to an object called rf.test.pred.
  CorrectAnswer: rf.test.pred <- predict(rf.lalonde, lalonde.test)
  AnswerTests: any_of_exprs('rf.test.pred <- predict(rf.lalonde, lalonde.test)')
  Hint: Remember we put the model and the test set as arguments in predict().

- Class: cmd_question
  Output: Now we can use the calc_rmse function (already loaded) to calculate the training set error.
      Save the result to an object called rmse.rf.test.
  CorrectAnswer: rmse.rf.test <- calc_rmse(lalonde.test$re75, rf.test.pred)
  AnswerTests: any_of_exprs('rmse.rf.test <- calc_rmse(lalonde.test$re75, rf.test.pred)')
  Hint: Change this, calc_rmse(ytrue, ypred)


- Class: text
  Output: Let's compare the test set error of our two models. 
      For our bagging model, the rmse is approximately 1844.
      For our random forest model, the rmse is approximately 1833. 
      Though the difference is small here, it demonstrates that usually the random forest model outperforms the decision tree and the bagging models in external test sets.
      This means that there is a lower risk of overfitting in the model.

- Class: mult_question
  Output: "Would you like the code-generator to generate a code for you?"
  AnswerChoices: Yes;No
  CorrectAnswer: NULL
  AnswerTests: cs112_credit(941, "s10")
  Hint:
