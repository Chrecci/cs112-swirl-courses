- Class: meta
  Course: CS112
  Lesson: Session 10 Random Forests
  Author: Michael Chen
  Type: Standard
  Organization: Minerva University
  Version: 2.4.5
  
- Class: text
  Output: Welcome to the Swirl lesson for CS112 Session 10, Random Forests. 
      Please note that you can use the function rpt() to go back to the
      previous question. To end your Swirl session, you can type bye().
      After you exit a lesson, you can always resume a lesson by typing 
      swirl() again. However, make sure you use the same name you always use!
      
- Class: text
  Output: In this lesson, we will build upon the last one about decision trees, and grow a forest of decision trees!
      If you have not done the previous lesson, it is strong recommended to do that before starting this one.

- Class: cmd_question
  Output: Let's start by loading the necessary library for growing a random forest. 
      Load the "randomForest" library. Install it if you haven't done so.
  CorrectAnswer: library(randomForest)
  AnswerTests: any_of_exprs('library(randomForest)')
  Hint: To install, install.packages("randomForest"). To load, library(randomForest).

- Class: text
  Output: Our trusty lalonde dataset is automatically loaded in the beginning of the session.
      It is also appropriately separated into training set (n=345) and test set (n=100). 

- Class: cmd_question
  Output: Let's set a seed to make sure our trees turns out the same way. Set seed to 112.
  CorrectAnswer: set.seed(112)
  AnswerTests: any_of_exprs('set.seed(112)')
  Hint: set.seed(112)

- Class: cmd_question
  Output: Use the randomForest() function, which is similar in format as tree(), 
      to apply bagging. Note that the function randomForest() can be used for bagging
      as well. Save the forest to an object called bag.lalonde.
      Here's the expression we'll use, randomForest(re75~.-re78, data=lalonde.train, mtry=10, importance = TRUE).
      The argument 'mtry' specifies the number of variables that can be used to classify at each node.
      The argument 'importance' allows us to see the relative influence of each variable.
      Note that we're using re75 as the dependent variable and everything else except
      re78 as independent variables.
  CorrectAnswer: bag.lalonde <- randomForest(re75~.-re78, data=lalonde.train, mtry=10, importance = TRUE)
  AnswerTests: any_of_exprs('bag.lalonde <- randomForest(re75~.-re78, data=lalonde.train, mtry=10, importance = TRUE)')
  Hint: Just copy and paste the expression and save it to an object called bag.lalonde.

- Class: cmd_question
  Output: Let's take a look at our bagging model.
  CorrectAnswer: bag.lalonde
  AnswerTests: any_of_exprs('bag.lalonde')
  Hint: Just type bag.lalonde.

- Class: text
  Output: We can see that we grew 500 trees, and at each split, all 10 independent variables are considered.
      The mean squared error and R^2 value are also reported.

- Class: cmd_question
  Output:  Let's see how our model performs predicting the test data. Save the 
      predictions to an object called bag.test.pred. The test data is called 
      'lalonde.test'. You don't need to specify any other argument.
  CorrectAnswer: bag.test.pred <- predict(bag.lalonde, lalonde.test)
  AnswerTests: any_of_exprs('bag.test.pred <- predict(bag.lalonde, lalonde.test)')
  Hint: Remember we put the model and the test set as arguments in predict().

- Class: cmd_question
  Output: Let's calculate the rmse value of this model using the test set. 
      Use the calc_rmse function (already loaded) to calculate the test set error.
      Save the result to an object called rmse.bag.test. You have to first enter
      the observed values and then the predicted values.
  CorrectAnswer: rmse.bag.test <- calc_rmse(lalonde.test$re75, bag.test.pred)
  AnswerTests: any_of_exprs('rmse.bag.test <- calc_rmse(lalonde.test$re75, bag.test.pred)')
  Hint: Change this, calc_rmse(ytrue, ypred). Also did you save the results to 
      an object called rmse.bag.test?


- Class: mult_question
  Output: Now, we will apply random forests to the dataset. Note that in the 
      lalonde dataset, there are 12 variables in total. If we use re75 as the 
      dependent variable and the rest (except re78) as independent variables, 
      how many variables can we use at each split?
      (Note that the heuristic is to use sqrt(number of variables))
  AnswerChoices: 11;10;9;4;3;2
  CorrectAnswer: "3"
  AnswerTests: omnitest(correctVal='3')
  Hint: Square root of 10 is ~3. 

- Class: cmd_question
  Output: Now, use the randomForest() function to make a random forest.
      Save the forest to an object called rf.lalonde.
      (Hint, you only need to change one parameter in from the bagging model.)
  CorrectAnswer: rf.lalonde <- randomForest(re75~.-re78, data=lalonde.train, mtry=3, importance = TRUE)
  AnswerTests: any_of_exprs('rf.lalonde <- randomForest(re75~.-re78, data=lalonde.train, mtry=3, importance = TRUE)')
  Hint: Change the expression and save it to an object called rf.lalonde.

- Class: cmd_question
  Output: Let's take a look at our random forest model.
  CorrectAnswer: rf.lalonde
  AnswerTests: any_of_exprs('rf.lalonde')
  Hint: Just type rf.lalonde.

- Class: cmd_question
  Output: We can also look at the respective importance of each variable. 
      Enter importance(rf.lalonde). The %IncMSE value indicates the change 
      in MSE when the variable is removed. Thus, the larger the %IncMSE, 
      the more important the variable.
  CorrectAnswer: importance(rf.lalonde)
  AnswerTests: any_of_exprs('importance(rf.lalonde)')
  Hint:

- Class: cmd_question
  Output: The importance can also be visualized by the varImpPlot(). Input varImpPlot(rf.lalonde)
  CorrectAnswer: varImpPlot(rf.lalonde)
  AnswerTests: any_of_exprs('varImpPlot(rf.lalonde)')
  Hint:

- Class: cmd_question
  Output: Let's calculate the rmse value of this model using the test set. First, let's generate the predictions.
      Save the predictions to an object called rf.test.pred.
  CorrectAnswer: rf.test.pred <- predict(rf.lalonde, lalonde.test)
  AnswerTests: any_of_exprs('rf.test.pred <- predict(rf.lalonde, lalonde.test)')
  Hint: Remember we put the model and the test set as arguments in predict().

- Class: cmd_question
  Output: Now we can use the calc_rmse function (already loaded) to calculate the training set error.
      Save the result to an object called rmse.rf.test.
  CorrectAnswer: rmse.rf.test <- calc_rmse(lalonde.test$re75, rf.test.pred)
  AnswerTests: any_of_exprs('rmse.rf.test <- calc_rmse(lalonde.test$re75, rf.test.pred)')
  Hint: Change this, calc_rmse(ytrue, ypred)

- Class: text
  Output: Good job! You've learned the basics of bagging and random forest models. 
      Now you can show this off to your friends.

- Class: mult_question
  Output: "Would you like the code-generator to generate a code for you?"
  AnswerChoices: Yes;No
  CorrectAnswer: NULL
  AnswerTests: cs112_credit(941, "s10")
  Hint:
